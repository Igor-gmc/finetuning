"""
–ì–ª–∞–≤–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è Fine-tuning –∏ RAG –ø–æ–¥—Ö–æ–¥–æ–≤
"""

import os
import warnings

import torch
from dotenv import load_dotenv
from openai import OpenAI
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import pandas as pd

from fine_tuning_fixed import fine_tune_qwen
from rag_system import create_rag_database

warnings.filterwarnings("ignore")


# –°–∏—Å—Ç–µ–º–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è
SYSTEM_INSTRUCTION = """–í—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –∫–æ–º–ø–∞–Ω–∏–∏ –£–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç –ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞.
–ö–æ–º–ø–∞–Ω–∏—è –∑–∞–Ω–∏–º–∞–µ—Ç—Å—è –ø—Ä–æ–¥–∞–∂–µ–π –∫—É—Ä—Å–æ–≤ –ø–æ AI.
–û—Ç–≤–µ—Ç—å—Ç–µ –Ω–∞ –≤–æ–ø—Ä–æ—Å (–∑–∞–ø—Ä–æ—Å) —Ç–∞–∫, —á—Ç–æ–±—ã —á–µ–ª–æ–≤–µ–∫ –∑–∞—Ö–æ—Ç–µ–ª –ø–æ—Å–ª–µ –æ—Ç–≤–µ—Ç–∞ –∫—É–ø–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ.
–ï—Å–ª–∏ –≤—ã –Ω–µ –∑–Ω–∞–µ—Ç–µ –æ—Ç–≤–µ—Ç–∞, –ø—Ä–æ—Å—Ç–æ —Å–∫–∞–∂–∏—Ç–µ, —á—Ç–æ –Ω–µ –∑–Ω–∞–µ—Ç–µ, –Ω–µ –ø—ã—Ç–∞–π—Ç–µ—Å—å –≤—ã–¥—É–º—ã–≤–∞—Ç—å –æ—Ç–≤–µ—Ç."""

# –¢–µ—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã
QUESTIONS = [
    "–ü—Ä–∏–≤–µ—Ç, –∞ —á—Ç–æ —Ç–∞–∫–æ–µ –∫—É—Ä—Å CHATGPT PROFESSIONAL –∏ –µ–≥–æ —Å—Ç–æ–∏–º–æ—Å—Ç—å?",
    "–ü—Ä–∏–≤–µ—Ç, –∞ –∫–∞–∫–∏–µ –∫—É—Ä—Å—ã –µ—Å—Ç—å –ø–æ Python?",
    "–ö–∞–∫–∏–µ –µ—Å—Ç—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫—É—Ä—Å–æ–≤ —É –£–ò–ò?",
    "–°–∫–æ–ª—å–∫–æ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —É—á–∏—Ç—å—Å—è –Ω–∞ –∫—É—Ä—Å–µ –ø–æ GPT?",
    "–ï—Å—Ç—å —Ç—Ä—É–¥–æ—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è?",
]


class RAGGenerator:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ RAG + Qwen (–±–µ–∑ fine-tuning)"""

    def __init__(self, rag_system, hf_token):
        self.rag = rag_system
        self.hf_token = hf_token
        self.model = None
        self.tokenizer = None
        self._load_base_model()

    def _load_base_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ Qwen –±–µ–∑ fine-tuning"""
        import platform

        print("\nüîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ Qwen –¥–ª—è RAG...", flush=True)
        from tqdm import tqdm

        model_name = "Qwen/Qwen2.5-1.5B-Instruct"
        is_macos = platform.system() == "Darwin"

        if is_macos:
            print(
                "‚ö†Ô∏è INT4 –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞ –Ω–∞ macOS (bitsandbytes —Ç—Ä–µ–±—É–µ—Ç CUDA)",
                flush=True,
            )

        with tqdm(total=100, desc="–ó–∞–≥—Ä—É–∑–∫–∞ Qwen", unit="%") as pbar:
            pbar.update(10)

            self.tokenizer = AutoTokenizer.from_pretrained(
                model_name, token=self.hf_token, trust_remote_code=True
            )
            pbar.update(30)

            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            pbar.update(10)

            if is_macos:
                # –ù–∞ macOS –±–µ–∑ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype=torch.float16,
                    device_map="auto",
                    token=self.hf_token,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True,
                )
            else:
                # –ù–∞ Linux/Windows —Å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π
                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                )
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    quantization_config=bnb_config,
                    device_map="auto",
                    token=self.hf_token,
                    trust_remote_code=True,
                )
            pbar.update(50)

        print("‚úì –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!", flush=True)

    def generate_answer(self, question):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RAG"""
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ RAG
        context = self.rag.get_context_for_question(question, top_k=3)

        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
        messages = [
            {"role": "system", "content": SYSTEM_INSTRUCTION},
            {"role": "user", "content": f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}\n\n–í–æ–ø—Ä–æ—Å: {question}"},
        ]

        prompt = self.tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                top_p=0.9,
                pad_token_id=self.tokenizer.pad_token_id,
            )

        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
        if "assistant" in response.lower():
            parts = response.split("assistant")
            if len(parts) > 1:
                response = parts[-1].strip()

        return response


class FineTunedGenerator:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ fine-tuned –º–æ–¥–µ–ª–∏"""

    def __init__(self, model_path, hf_token):
        self.model_path = model_path
        self.hf_token = hf_token
        self.model = None
        self.tokenizer = None
        self._load_finetuned_model()

    def _load_finetuned_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ fine-tuned –º–æ–¥–µ–ª–∏"""
        import platform

        print("\nüîÑ –ó–∞–≥—Ä—É–∑–∫–∞ fine-tuned –º–æ–¥–µ–ª–∏...", flush=True)
        from tqdm import tqdm

        base_model_name = "Qwen/Qwen2.5-1.5B-Instruct"
        is_macos = platform.system() == "Darwin"

        if is_macos:
            print(
                "‚ö†Ô∏è INT4 –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞ –Ω–∞ macOS (bitsandbytes —Ç—Ä–µ–±—É–µ—Ç CUDA)",
                flush=True,
            )

        with tqdm(total=100, desc="–ó–∞–≥—Ä—É–∑–∫–∞ Fine-tuned", unit="%") as pbar:
            pbar.update(10)

            # –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ —Å/–±–µ–∑ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏
            if is_macos:
                base_model = AutoModelForCausalLM.from_pretrained(
                    base_model_name,
                    torch_dtype=torch.float16,
                    device_map="auto",
                    token=self.hf_token,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True,
                )
            else:
                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                )
                base_model = AutoModelForCausalLM.from_pretrained(
                    base_model_name,
                    quantization_config=bnb_config,
                    device_map="auto",
                    token=self.hf_token,
                    trust_remote_code=True,
                )
            pbar.update(40)

            # –ó–∞–≥—Ä—É–∑–∫–∞ LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞
            self.model = PeftModel.from_pretrained(base_model, self.model_path)
            pbar.update(30)

            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path, trust_remote_code=True
            )
            pbar.update(15)

            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            pbar.update(5)

        print("‚úì Fine-tuned –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!", flush=True)

    def generate_answer(self, question):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º fine-tuned –º–æ–¥–µ–ª–∏"""
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç (–±–µ–∑ —è–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –º–æ–¥–µ–ª—å —É–∂–µ –æ–±—É—á–µ–Ω–∞)
        messages = [
            {"role": "system", "content": SYSTEM_INSTRUCTION},
            {"role": "user", "content": question},
        ]

        prompt = self.tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                top_p=0.9,
                pad_token_id=self.tokenizer.pad_token_id,
            )

        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
        if "assistant" in response.lower():
            parts = response.split("assistant")
            if len(parts) > 1:
                response = parts[-1].strip()

        return response


def analyze_with_gpt4(results_text, openai_api_key):
    """
    –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é GPT-4o-mini

    Args:
        results_text: –¢–µ–∫—Å—Ç —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±–µ–∏—Ö –º–æ–¥–µ–ª–µ–π
        openai_api_key: API –∫–ª—é—á OpenAI

    Returns:
        –¢–µ–∫—Å—Ç –∞–Ω–∞–ª–∏–∑–∞
    """
    print("\nüîÑ –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ —Å GPT-4o-mini...")
    from tqdm import tqdm

    with tqdm(total=100, desc="–ê–Ω–∞–ª–∏–∑ GPT-4o-mini", unit="%") as pbar:
        client = OpenAI(api_key=openai_api_key)
        pbar.update(10)

        prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤ –¥–≤—É—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –∫ —Å–æ–∑–¥–∞–Ω–∏—é AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –¥–ª—è –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏—è –∫—É—Ä—Å–æ–≤ –£–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞ –ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞:

1. RAG (Retrieval-Augmented Generation) + Qwen2.5-1.5B-Instruct –±–µ–∑ Fine-tuning
2. Qwen2.5-1.5B-Instruct + Fine-tuning

–ö—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏:
- –¢–æ—á–Ω–æ—Å—Ç—å –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤
- –ù–∞–ª–∏—á–∏–µ –º–æ—Ç–∏–≤–∏—Ä—É—é—â–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–æ–∫—É–ø–∫–∏ –∫—É—Ä—Å–æ–≤
- –ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å –∏ —Å–≤—è–∑–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞
- –ü–æ–ª–Ω–æ—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
- –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å—Ç–∏–ª—é –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –º–µ–Ω–µ–¥–∂–µ—Ä–∞

–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:

{results_text}

–ü—Ä–µ–¥–æ—Å—Ç–∞–≤—å –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑:
1. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ –∫–∞–∂–¥–æ–º—É –≤–æ–ø—Ä–æ—Å—É
2. –û–±—â–∏–µ —Å–∏–ª—å–Ω—ã–µ –∏ —Å–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –∫–∞–∂–¥–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞
3. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è, –∫–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –ª—É—á—à–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –¥–∞–Ω–Ω–æ–π –∑–∞–¥–∞—á–∏
4. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ —É–ª—É—á—à–µ–Ω–∏—é"""
        pbar.update(20)

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –æ—Ü–µ–Ω–∫–µ –∫–∞—á–µ—Å—Ç–≤–∞ AI –º–æ–¥–µ–ª–µ–π –∏ NLP —Å–∏—Å—Ç–µ–º.",
                },
                {"role": "user", "content": prompt},
            ],
            temperature=0.7,
            max_tokens=2000,
        )
        pbar.update(60)

        analysis = response.choices[0].message.content
        pbar.update(10)

    print("‚úì –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
    return analysis


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("=" * 80)
    print("Fine-tuning vs RAG: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ–¥—Ö–æ–¥–æ–≤ –¥–ª—è —á–∞—Ç-–±–æ—Ç–∞ –£–ò–ò")
    print("=" * 80)

    # –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
    load_dotenv()
    hf_token = os.getenv("HF_TOKEN")
    openai_api_key = os.getenv("OPENAI_API_KEY")

    csv_path = "selected_qa_full.csv"
    finetuned_model_path = "./qwen_finetuned"
    # –î–ª—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ –æ–≥—Ä–∞–Ω–∏—á–∏–º —á–∏—Å–ª–æ –ø—Ä–∏–º–µ—Ä–æ–≤ (—á—Ç–æ–±—ã –Ω–µ —Ç—è–Ω—É—Ç—å –≤–µ—Å—å –±–æ–ª—å—à–æ–π csv)
    MAX_SAMPLES = 50  # —É–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞
    subset_csv_path = "selected_qa_subset.csv"

    if MAX_SAMPLES is not None:
        if not os.path.exists(subset_csv_path):
            print(
                f"–°–æ–∑–¥–∞—é –ø–æ–¥–≤—ã–±–æ—Ä–∫—É –∏–∑ {csv_path} —Å {MAX_SAMPLES} –ø—Ä–∏–º–µ—Ä–∞–º–∏ -> {subset_csv_path}"
            )
            df = pd.read_csv(csv_path, nrows=MAX_SAMPLES)
            df.to_csv(subset_csv_path, index=False)
        csv_path_to_use = subset_csv_path
    else:
        csv_path_to_use = csv_path

    # –®–∞–≥ 1: –°–æ–∑–¥–∞–Ω–∏–µ RAG –±–∞–∑—ã
    print("\n" + "=" * 80)
    print("–®–∞–≥ 1/4: –°–æ–∑–¥–∞–Ω–∏–µ RAG –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö")
    print("=" * 80)
    rag_system = create_rag_database(csv_path_to_use)

    # –®–∞–≥ 2: Fine-tuning –º–æ–¥–µ–ª–∏
    print("\n" + "=" * 80)
    print("–®–∞–≥ 2/4: Fine-tuning –º–æ–¥–µ–ª–∏ Qwen")
    print("=" * 80)

    def finetuned_is_complete(path):
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ LoRA/PEFT
        return os.path.isdir(path) and os.path.exists(
            os.path.join(path, "adapter_config.json")
        )

    if not finetuned_is_complete(finetuned_model_path):
        print(
            f"Fine-tuned –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –∏–ª–∏ –Ω–µ–ø–æ–ª–Ω–∞—è: {finetuned_model_path}. –ó–∞–ø—É—Å–∫–∞—é –¥–æ–æ–±—É—á–µ–Ω–∏–µ..."
        )
        fine_tune_qwen(
            csv_path=csv_path_to_use,
            hf_token=hf_token,
            system_instruction=SYSTEM_INSTRUCTION,
            output_dir=finetuned_model_path,
            max_samples=MAX_SAMPLES,
        )
    else:
        print(f"‚úì Fine-tuned –º–æ–¥–µ–ª—å —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {finetuned_model_path}")

    # –®–∞–≥ 3: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤
    print("\n" + "=" * 80)
    print("–®–∞–≥ 3/4: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤ –æ–±–æ–∏–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏")
    print("=" * 80)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤
    rag_generator = RAGGenerator(rag_system, hf_token)
    finetuned_generator = FineTunedGenerator(finetuned_model_path, hf_token)

    # RAG –æ—Ç–≤–µ—Ç—ã
    print("\nüìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é RAG...")
    rag_results = []
    from tqdm import tqdm

    for i, question in enumerate(
        tqdm(QUESTIONS, desc="RAG –≥–µ–Ω–µ—Ä–∞—Ü–∏—è", unit="–≤–æ–ø—Ä–æ—Å"), 1
    ):
        answer = rag_generator.generate_answer(question)
        rag_results.append((question, answer))

    # Fine-tuned –æ—Ç–≤–µ—Ç—ã
    print("\nüìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é Fine-tuned –º–æ–¥–µ–ª–∏...")
    finetuned_results = []
    for i, question in enumerate(
        tqdm(QUESTIONS, desc="Fine-tuned –≥–µ–Ω–µ—Ä–∞—Ü–∏—è", unit="–≤–æ–ø—Ä–æ—Å"), 1
    ):
        answer = finetuned_generator.generate_answer(question)
        finetuned_results.append((question, answer))

    # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    results_text = "RAG (Retrieval-Augmented Generation) Qdrant + Qwen2.5-1.5B-Instruct (—Å INT4 –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π) –±–µ–∑ Fine-tuning\n"
    results_text += "=" * 80 + "\n"

    for i, (question, answer) in enumerate(rag_results, 1):
        results_text += f"\n–í–æ–ø—Ä–æ—Å {i}: {question}\n"
        results_text += f"–û—Ç–≤–µ—Ç {i}: {answer}\n"
        results_text += "=" * 80 + "\n"

    results_text += "\n\nQwen2.5-1.5B-Instruct (—Å INT4 –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π) + Fine-tuning –Ω–∞ –¥–∞—Ç–∞ —Å–µ—Ç–µ selected_qa_full.csv\n"
    results_text += "=" * 80 + "\n"

    for i, (question, answer) in enumerate(finetuned_results, 1):
        results_text += f"\n–í–æ–ø—Ä–æ—Å {i}: {question}\n"
        results_text += f"–û—Ç–≤–µ—Ç {i}: {answer}\n"
        results_text += "=" * 80 + "\n"

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª
    with open("result.txt", "w", encoding="utf-8") as f:
        f.write(results_text)

    print("\n‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ result.txt")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ DataFrame
    print("\nüìä –≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ DataFrame...")

    # DataFrames –¥–ª—è RAG –∏ Fine-tuned —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    rag_df = pd.DataFrame(
        [(i + 1, q, a) for i, (q, a) in enumerate(rag_results)],
        columns=["‚Ññ –≤–æ–ø—Ä–æ—Å–∞", "–í–æ–ø—Ä–æ—Å", "–û—Ç–≤–µ—Ç RAG"],
    )

    finetuned_df = pd.DataFrame(
        [(i + 1, q, a) for i, (q, a) in enumerate(finetuned_results)],
        columns=["‚Ññ –≤–æ–ø—Ä–æ—Å–∞", "–í–æ–ø—Ä–æ—Å", "–û—Ç–≤–µ—Ç Fine-tuned"],
    )

    # –û–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π DataFrame –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    comparison_df = pd.DataFrame(
        {
            "‚Ññ –≤–æ–ø—Ä–æ—Å–∞": [i + 1 for i in range(len(QUESTIONS))],
            "–í–æ–ø—Ä–æ—Å": QUESTIONS,
            "–û—Ç–≤–µ—Ç RAG": [a for _, a in rag_results],
            "–û—Ç–≤–µ—Ç Fine-tuned": [a for _, a in finetuned_results],
        }
    )

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ CSV
    rag_df.to_csv("results_rag.csv", index=False, encoding="utf-8")
    finetuned_df.to_csv("results_finetuned.csv", index=False, encoding="utf-8")
    comparison_df.to_csv("results_comparison.csv", index=False, encoding="utf-8")

    print("‚úì DataFrame —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
    print("  - results_rag.csv")
    print("  - results_finetuned.csv")
    print("  - results_comparison.csv")

    # –í—ã–≤–æ–¥ —Ç–∞–±–ª–∏—Ü
    print("\n" + "=" * 80)
    print("RAG –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print("=" * 80)
    print(rag_df.to_string(index=False))

    print("\n" + "=" * 80)
    print("FINE-TUNED –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print("=" * 80)
    print(finetuned_df.to_string(index=False))

    # –®–∞–≥ 4: –ê–Ω–∞–ª–∏–∑ —Å –ø–æ–º–æ—â—å—é GPT-4o-mini
    print("\n" + "=" * 80)
    print("–®–∞–≥ 4/4: –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é GPT-4o-mini")
    print("=" * 80)

    analysis = analyze_with_gpt4(results_text, openai_api_key)

    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ –≤ —Ñ–∞–π–ª
    with open("result.txt", "a", encoding="utf-8") as f:
        f.write("\n\n" + "=" * 80 + "\n")
        f.write("–ê–ù–ê–õ–ò–ó –ö–ê–ß–ï–°–¢–í–ê –û–¢–í–ï–¢–û–í (GPT-4o-mini)\n")
        f.write("=" * 80 + "\n\n")
        f.write(analysis if analysis else "–ê–Ω–∞–ª–∏–∑ –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω")

    print("\n‚úì –ê–Ω–∞–ª–∏–∑ –¥–æ–±–∞–≤–ª–µ–Ω –≤ result.txt")

    print("\n" + "=" * 80)
    print("‚úÖ –í–°–ï –ó–ê–î–ê–ß–ò –í–´–ü–û–õ–ù–ï–ù–´!")
    print("=" * 80)
    print("\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª–µ: result.txt")
    print("\n–í–∫–ª—é—á–∞–µ—Ç:")
    print("  - –û—Ç–≤–µ—Ç—ã RAG —Å–∏—Å—Ç–µ–º—ã")
    print("  - –û—Ç–≤–µ—Ç—ã Fine-tuned –º–æ–¥–µ–ª–∏")
    print("  - –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ—Ç GPT-4o-mini")


if __name__ == "__main__":
    main()
